---
title: "Unsupervised Learning"
output: github_document
---

## How to get set up for this workshop

### Our virtual space:

[http://backchannelchat.com/chat/parmw](http://backchannelchat.com/chat/parmw)

### Getting this document on your computer:

1. Go to the GitHub repository here: [https://github.com/davidklaing/data_science_workshops](https://github.com/davidklaing/data_science_workshops)
2. Click the green button on the right that says "Clone or download".
3. Click "Download ZIP". (If you're proficient with git, feel free to clone the repository.)
4. Create a folder on your computer to store your work, and store your ZIP file there.
5. Double-click your ZIP file to unzip it and get all the code.

### Getting R and Rstudio

1. Download and install R from here: [http://cran.stat.sfu.ca/](http://cran.stat.sfu.ca/).
2. Download and install RStudio Desktop (Open Source Edition) from here: [https://www.rstudio.com/products/rstudio/#Desktop](https://www.rstudio.com/products/rstudio/#Desktop).

### Getting ready to play!

1. In RStudio, open `unsupervised_learning.Rmd`, a file in `YOUR_FOLDER/data_science_workshops/unsupervised_learning/src/`. (That's this file!)
2. In the code snippet below, remove the hashtags from both lines, and click the green "play" button on the right to install `dplyr` and `ggplot2`, the two packages you'll need for the workshop.

```{r}
#install.packages("dplyr")
#install.packages("ggplot2")

library(dplyr)
library(ggplot2)
library(purrr)
```

### Having installation problems?

Option 1: Ask for help! We have volunteers who can help troubleshoot.

Option 2a: Find a partner and follow along together — most of the exercises can be done collaboratively. Your installation problem is almost certainly solvable — we just might not have time today.

Option 2b: Use [this datacamp light](https://cdn.datacamp.com/dcl/standalone-example.html) page to write and execute code in your browser. (A couple caveats: doesn't come with the titanic data, and might end up distracting you from the lesson. If this is your first time programming, I recommend option 2a.)

## Rstudio review

Executing code: move your cursor to the line you want to execute, and hit CTRL+ENTER

```{r}
print("hello women who code Vancouver!")
1 + 1
```

Looking up documentation about a function or object: prepend the name of the object with a question mark, and execute that line.

```{r}
?print
```

```{r}
set.seed(3)

generate_height <- function(species) {
  if (species == "wizard") {
    max(0, rnorm(1, 1.8, 0.3) + rnorm(1, 0, 0.4))
  } else if (species == "dwarf") {
    max(0, rnorm(1, 1.2, 0.2) + rnorm(1, 0, 0.4))
  } else if (species == "giant") {
    max(0, rnorm(1, 2.6, 0.5) + rnorm(1, 0, 0.4))
  }
}

generate_magical_powers <- function(species) {
  if (species == "wizard") {
    max(0, rnorm(1, 8, 1.2))
  } else if (species == "dwarf") {
    max(0, rnorm(1, 3.5, 0.6))
  } else if (species == "giant") {
    max(0, rnorm(1, 1, 0.7))
  }
}

magical_data <- data_frame(
  species = sample(c("wizard", "dwarf", "giant"), 100, replace = TRUE)
) %>% 
  mutate(
    height = map_dbl(species, generate_height),
    magical_powers = map_dbl(species, generate_magical_powers)
  )

unsupervised_magical_data <- magical_data %>% 
  select(height, magical_powers)

magical_data %>% 
  ggplot() +
  geom_point(
    aes(
      x = height,
      y = magical_powers,
      color = species
    )
  )

unsupervised_magical_data %>% 
  ggplot() +
  geom_point(
    aes(
      x = height,
      y = magical_powers
    )
  )
```

```{r}
?kmeans

unsupervised_magical_data

kmeans_result <- kmeans(unsupervised_magical_data, 3)

kmeans_result

kmeans_result$cluster

magical_data %>% 
  mutate(cluster = kmeans_result$cluster) %>% 
  mutate(true_cluster = case_when(
    species == "dwarf" ~ 1,
    species == "giant" ~ 3,
    species == "wizard" ~ 2
  )) %>% 
  mutate(misclassified = cluster != true_cluster) %>% 
  ggplot() +
  geom_point(
    aes(
      x = height,
      y = magical_powers
    )
  )
```

# Implement k-means!

Steps:

1. Choose k
2. Initialize k random centroids (can choose points)
3. Compute the distance from each point to the k centroids
4. Assign each point to a group corresponding to its closest centroid
5. Reinitialize the k centroids by computing the mean of all points in its corresponding cluster
6. Repeat steps 4 and 5 until the points no longer change groups.

# Levels of difficulty

Easiest: 3-means with a defined number of iterations

Harder: 3-means with convergence criterion

Hardest: k-means with convergence criterion

```{r}
?sample
?pmap

```

```{r}
compute_distance_to_centroid <- function(x1, x2, y1, y2) {
  sqrt((x2 - x1)^2 + (y2 - y1)^2)
}

compute_distance_to_centroid(1,4,1,5)
```


```{r}
assign_to_cluster <- function(x_val, y_val, centroid_df) {
  centroid_df %>% 
    mutate(
      distance_to_centroid = pmap_dbl(
        list(x_val, x, y_val, y),
        compute_distance_to_centroid
      )
    ) %>% 
    arrange(distance_to_centroid) %>% 
    slice(1) %>% 
    pull(cluster)
}

assign_to_cluster(
  x_val = 1,
  y_val = 2,
  centroid_df = data_frame(
    x = c(5, 1, 9),
    y = c(6, 3, 2),
    cluster = c(1, 2, 3)
  )
)
```

```{r}
my_kmeans <- function(x, y, k, max_iter = 10) {
  df <- data_frame(x, y)
  initial_centroid_indices <- sample(seq(1:nrow(df)), size = k)
  initial_centroids <- df %>% 
    slice(initial_centroid_indices) %>% 
    mutate(cluster = 1:k)
  initial_clusters <- df %>%
    mutate(
      cluster = pmap_int(
        list(x_val = x, y_val = y),
        assign_to_cluster,
        centroid_df = initial_centroids
      )
    )
  new_centroids <- initial_clusters %>% 
    group_by(cluster) %>% 
    summarise(
      x = mean(x),
      y = mean(y)
    ) %>% 
    select(x, y, cluster)
  old_centroids <- initial_centroids
  counter <- 1
  while (!all(new_centroids == old_centroids) & counter < max_iter) {
    old_centroids <- new_centroids
    new_clusters <- df %>% 
      mutate(
        cluster = pmap_int(
          list(x_val = x, y_val = y),
          assign_to_cluster,
          centroid_df = new_centroids
        )
      )
    new_centroids <- new_clusters %>% 
      group_by(cluster) %>% 
      summarise(
        x = mean(x),
        y = mean(y)
      )
    counter <- counter + 1
  }
  return(new_clusters)
}
```

```{r}
learned_clusters <- my_kmeans(
  x = magical_data$height, 
  y = magical_data$magical_powers, 
  k = 4
) %>% 
  pull(cluster)

magical_data %>% 
  mutate(cluster = learned_clusters) %>% 
  ggplot() +
  geom_point(
    aes(
      x = height,
      y = magical_powers,
      color = factor(cluster)
    )
  )
```

```{r}
new_kmeans <- function(df, k) {
  
}

general_assign_to_cluster <- function(data_df, centers_df) {
  clusters <- expand.grid(
    point_index = 1:nrow(data_df),
    center_index = 1:nrow(centers_df)
  ) %>% 
    mutate(
      data_coordinates = map(point_index, get_coordinates, data_df),
      center_coordinates = map(center_index, get_coordinates, centers_df)
    ) %>% 
    mutate(
      distance = map2_dbl(
        data_coordinates,
        center_coordinates,
        get_general_distance
      )
    ) %>% 
    group_by(point_index) %>% 
    filter(distance == min(distance)) %>% 
    ungroup() %>% 
    arrange(point_index) %>% 
    pull(center_index)
  
  data_df %>% 
    mutate(cluster = clusters)
}

general_assign_to_cluster(
  unsupervised_magical_data, 
  slice(unsupervised_magical_data, sample(1:nrow(unsupervised_magical_data), 3))
)

get_coordinates <- function(index, df) {
  t(slice(df, index))
}

get_general_distance <- function(v1, v2) {
  sqrt(sum((v1 - v2)^2))
}

get_general_distance(v1 = c(0,1,1), v2 = c(1,4,5))

magical_data %>% slice(1) %>% select_if(is.numeric) %>% t()
```


# Recap

Workshop 1: data wrangling

Workshop 2: data visualization: [https://github.com/hadinh1306/workshop-materials](https://github.com/hadinh1306/workshop-materials)

Workshop 3: statistics

Workshop 4: supervised learning

Workshop 5: unsupervised learning

