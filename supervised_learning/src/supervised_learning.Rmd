---
title: "Supervised Learning"
output: github_document
---

```{r setup, include=FALSE}
#install.packages("knitr")
knitr::opts_chunk$set(echo = TRUE)
```

## How to get set up for this workshop

### Our virtual space:

[http://backchannelchat.com/Backchannel/l9yzy](http://backchannelchat.com/Backchannel/l9yzy)

### Getting this document on your computer:

1. Go to the GitHub repository here: [https://github.com/davidklaing/data_science_workshops](https://github.com/davidklaing/data_science_workshops)
2. Click the green button on the right that says "Clone or download".
3. Click "Download ZIP". (If you're proficient with git, feel free to clone the repository.)
4. Create a folder on your computer to store your work, and store your ZIP file there.
5. Double-click your ZIP file to unzip it and get all the code.

### Getting R and Rstudio

1. Download and install R from here: [http://cran.stat.sfu.ca/](http://cran.stat.sfu.ca/).
2. Download and install RStudio Desktop (Open Source Edition) from here: [https://www.rstudio.com/products/rstudio/#Desktop](https://www.rstudio.com/products/rstudio/#Desktop).

### Getting ready to play!

1. In RStudio, open `supervised_learning.Rmd`, a file in `YOUR_FOLDER/data_science_workshops/supervised_learning/src/`. (That's this file!)
2. In the code snippet below, remove the hashtags from both lines, and click the green "play" button on the right to install `dplyr`, `ggplot2`, `purrr`, and `Lahman`, the four packages you'll need for the workshop.

```{r}
#install.packages("dplyr")
#install.packages("ggplot2")
#install.packages("Lahman")
#install.packages("purrr")

library(dplyr)
library(ggplot2)
library(Lahman)
library(purrr)
```

### Having installation problems?

Option 1: Ask for help! We have volunteers who can help troubleshoot.

Option 2a: Find a partner and follow along together — most of the exercises can be done collaboratively. Your installation problem is almost certainly solvable — we just might not have time today.

Option 2b: Use [this datacamp light](https://cdn.datacamp.com/dcl/standalone-example.html) page to write and execute code in your browser. (A couple caveats: doesn't come with the titanic data, and might end up distracting you from the lesson. If this is your first time programming, I recommend option 2a.)

## Rstudio review

Executing code: move your cursor to the line you want to execute, and hit CTRL+ENTER

```{r}
print("hello!")
1+1
```

Looking up documentation about a function or object: prepend the name of the object with a question mark, and execute that line.

```{r}
print
```

## Supervised Learning

In most programming:

- Human writes a function, which allows the machine to take some data as an input and return some other data as an output.

For example:

```{r}
add_one <- function(x) {
  return(x + 1)
}

add_one(2)
add_one(5)
add_one(0)
```

In machine learning:

- Human has a bunch of input-output pairs (like 2 and 3, 5 and 6, and 0 and 1), but doesn't know how to write a function that will transform the inputs into the outputs. So the human gives the input-output pairs to a machine learning algorithm, which learns the function we want!

```{r}
learned_function <- function(inputs, outputs, new_input) {
  training_data <- data_frame(
    input = inputs,
    output = outputs
  )
  model <- lm(output~input, training_data)
  new_input_data <- data_frame(
    input = new_input
  )
  return(as.numeric(predict(model, new_input_data)))
}

learned_function(
  inputs = c(2, 5, 0),
  outputs = c(3, 6, 1),
  new_input = 3
)
```

Throughout this workshop, we'll work toward applying a machine learning algorithm like the one above. But we're going to start out by trying to learn the appropriate function with our own eyes. In other words, we are going to be the ones doing the supervising.

## Exercise

Based on the graph below, how would you predict future values of $y$ knowing only the corresponding values of $x$?

```{r}
df1_train <- data_frame(
  x = c(0,1.1,2,3.3,4.4,5,6.2,7.7,8.1,9),
  y = c(0,2.2,4,6.6,8.8,10,12.4,15.2,16.2,18)
)

df1_train %>% 
  ggplot() +
  geom_point(
    aes(x = x, y = y)
  ) +
  scale_y_continuous(
    breaks = seq(0,20,2)
  ) +
  scale_x_continuous(
    breaks = c(0,1,2,3,4,5,6,7,8,9,10)
  ) +
  labs(
    title = "Graph 1"
  )

df1_train %>% 
  ggplot() +
  geom_histogram(
    aes(x = y)
  )
```

Write your function in the code snippet below.

```{r}
predict1 <- function(x) {
  return(x*2)
}

df1_test <- data_frame(
  x = c(0.7, 3, 4, 5.7, 7.5)
) %>% 
  mutate(
    y = map_dbl(x, predict1),
    set = "test"
  )

df1_train %>%
  mutate(set = "train") %>% 
  bind_rows(df1_test) %>% 
  ggplot() +
  geom_point(
    aes(x = x, y = y, color = set)
  ) +
  scale_y_continuous(
    breaks = seq(0,20,2)
  ) +
  scale_x_continuous(
    breaks = c(0,1,2,3,4,5,6,7,8,9,10)
  ) +
  labs(
    title = "Graph 1"
  )
```

Complication:

- What if there are breaks in the distribution that make it impossible to represent the prediction function with a linear equation?

```{r}
df2_train <- data_frame(
  x = c(0,1.4,2,3.7,4,5.1,6,7.5,8.4,9.7),
  y = c(1,1,1,1,1,3,3,3,3,3)
)

df2_train %>% 
  ggplot() +
  geom_point(
    aes(x = x, y = y)
  ) +
  scale_y_continuous(
    limits = c(0,4),
    breaks = c(0,1,2,3,4)
  ) +
  scale_x_continuous(
    breaks = c(0,1,2,3,4,5,6,7,8,9,10)
  ) +
  labs(
    title = "Graph 2"
  )
```

See if you can represent this new dataset with a different function.

Hint: you will need to use an `if` clause.

```{r}
if (1 == 2) {
  print("hello")
} else {
  print("The first clause was false!")
}
```


```{r}
predict2 <- function(x) {
  if (x < 4.5) {
    return(1)
  } else {
    return(3)
  }
}

df2_test <- data_frame(
  x = c(1, 2.2, 3, 5.5, 7, 9)
) %>% 
  mutate(
    y = map_dbl(x, predict2),
    set = "test"
  )

df2_train %>%
  mutate(set = "train") %>% 
  bind_rows(df2_test) %>% 
  ggplot() +
  geom_point(
    aes(x = x, y = y, color = set)
  ) +
  scale_y_continuous(
    limits = c(0,4),
    breaks = c(0,1,2,3,4)
  ) +
  scale_x_continuous(
    breaks = c(0,1,2,3,4,5,6,7,8,9,10)
  ) +
  labs(
    title = "Graph 2 - with predictions"
  )
```

Complication:

- What if there is a source of error that causes equivalent inputs to have variable outputs?

```{r}
df3_train <- read.csv("../data/df3_train.csv")

df3_train %>%
  ggplot() +
  geom_point(
    aes(
      x = x,
      y = y
    )
  ) +
  labs(title = "Graph 3")
```

By eyeballing the data, write your function in the snippet below. Once you're done, think about how you might evaluate the quality of your function. Can you come up with a measurement that you could use to compare your function to one written by someone else?

```{r}
tania_predict <- function(x) {
  return(x*x)
}

cerize_predict <- function(x) {
  return(20*x - 100)
}

steffi_predict <- function(x) {
  return(x^2 - x*2)
}

df3_train %>% 
  mutate(
    tania_prediction = map_dbl(x, tania_predict),
    cerize_prediction = map_dbl(x, cerize_predict),
    steffi_prediction = map_dbl(x, steffi_predict)
  ) %>% 
  ggplot() +
  geom_point(aes(x = x, y = y), color = "red") +
  geom_point(aes(x = x, y = tania_prediction), color = "blue") +
  geom_point(aes(x = x, y = cerize_prediction), color = "pink") +
  geom_point(aes(x = x, y = steffi_prediction), color = "purple")

# How could you evaluate the quality of your function??
```

```{r}
df3_train %>% 
  mutate(
    tania_prediction = map_dbl(x, tania_predict),
    cerize_prediction = map_dbl(x, cerize_predict),
    steffi_prediction = map_dbl(x, steffi_predict)
  ) %>%
  mutate(
    tania_error = tania_prediction - y,
    cerize_error = cerize_prediction - y,
    steffi_error = steffi_prediction - y
  ) %>% 
  summarise(
    mean_squared_tania_error = mean(tania_error^2),
    mean_squared_cerize_error = mean(cerize_error^2),
    mean_squared_steffi_error = mean(steffi_error^2),
  )
```

```{r}
df3_test <- read.csv("../data/df3_test.csv")

df3_test %>% 
  ggplot() +
  geom_point(
    aes(x = x, y = y)
  )

df3_test %>% 
  mutate(
    tania_prediction = map_dbl(x, tania_predict),
    cerize_prediction = map_dbl(x, cerize_predict),
    steffi_prediction = map_dbl(x, steffi_predict)
  ) %>%
  mutate(
    tania_error = tania_prediction - y,
    cerize_error = cerize_prediction - y,
    steffi_error = steffi_prediction - y
  ) %>% 
  summarise(
    mean_squared_tania_error = mean(tania_error^2),
    mean_squared_cerize_error = mean(cerize_error^2),
    mean_squared_steffi_error = mean(steffi_error^2),
  )
```

Complication:

- What if there are non-linearities in the data that make it difficult to define your function mathematically?

```{r}
df4 <- data_frame(
  x = c(0,1.1,2,3.3,3.9,4.4,5,6.2,7.7,8.1,9),
  y = c(1.1,1.8,0.9,0.87,4.1,8.8,8.4,7.4,6,6.4,6.9)
)

df4 %>% 
  ggplot() +
  geom_point(
    aes(x = x, y = y)
  ) +
  scale_y_continuous(
    breaks = seq(0,20,2)
  ) +
  scale_x_continuous(
    breaks = c(0,1,2,3,4,5,6,7,8,9,10)
  ) +
  labs(
    title = "Graph 4"
  )
```

Further complications:

- What if there are multiple predictor variables, making it impossible to visualize the whole system at once?

Say you want to predict a baseball player's salary using just their batting statistics:

```{r}
baseball_stats <- Batting %>%
  inner_join(Salaries) %>% 
  filter(yearID == 2005)

?Batting

baseball_stats
```

```{r}
set.seed(1)

training_set_players <- sample(baseball_stats$playerID, 600)

training_set <- baseball_stats %>% 
  filter(playerID %in% training_set_players)

test_set <- baseball_stats %>% 
  filter(!playerID %in% training_set_players)
```

```{r}
training_set %>%
  ggplot() +
  geom_point(aes(x = BB, y = salary))

salary_model <- lm(formula = salary~G+HR+H+IBB, data = training_set)

salary_model

summary(salary_model)
```


```{r}
training_set_with_predictions <- training_set %>%
  mutate(
    predicted_salary = predict(
      salary_model, 
      training_set
    )
  )

training_set_with_predictions %>% 
  mutate(
    prediction_error = predicted_salary - salary
  ) %>% 
  summarise(
    mean_squared_error = mean(prediction_error^2, na.rm = TRUE)
  )

training_set_with_predictions %>% 
  ggplot() +
  geom_point(
    aes(x = BB, y = salary), 
    color = "red", 
    alpha = 0.5
  ) +
  geom_point(
    aes(x = BB, y = predicted_salary), 
    color = "blue", 
    alpha = 0.5
  ) +
  labs(title = "Predictions on the training set")
```

```{r}
test_set_with_predictions <- test_set %>%
  mutate(
    predicted_salary = predict(salary_model, test_set)
  )

test_set_with_predictions %>% 
  mutate(
    prediction_error = predicted_salary - salary
  ) %>% 
  summarise(
    mean_squared_error = mean(prediction_error^2, na.rm = TRUE)
  )

test_set_with_predictions %>% 
  ggplot() +
  geom_point(
    aes(x = BB, y = salary), 
    color = "red", 
    alpha = 0.5
  ) +
  geom_point(
    aes(x = BB, y = predicted_salary),
    color = "blue", 
    alpha = 0.5
  ) +
  labs(title = "Predictions on the test set")
```

```{r}
zeinab_model <- lm(salary~BB + stint + G + AB + H + X2B + X3B + CS + IBB, training_set)

steffi_model <- lm(salary~(H+HR+X2B+X3B)/AB, training_set)

test_set %>%
  mutate(
    zeinab_prediction = predict(zeinab_model, test_set),
    steffi_prediction = predict(steffi_model, test_set)
  ) %>% 
  mutate(
    zeinab_test_error = zeinab_prediction - salary,
    steffi_test_error = steffi_prediction - salary
  ) %>% 
  summarise(
    zeinab_mean_squared_error = mean(zeinab_test_error^2),
    steffi_mean_squared_error = mean(steffi_test_error^2)
  )

training_set %>%
  mutate(
    prediction = predict(salary_model, training_set)
  ) %>% 
  ggplot() +
  geom_point(
    aes(
      x = prediction,
      y = salary
    ),
    alpha = 0.5
  )

summary(steffi_model)
```

sklearn
kaggle.com

